{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPL2TBJIzGGP4YbvMY2hRiv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoospark/pub_dfc605/blob/master/dfc605_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLg3F4QbaOZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "\n",
        "import pandas as pd  \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "# 네이버 플레이스 API를 호출하기 위한 기본 주소\n",
        "source_url = \"https://store.naver.com/sogum/api/businesses?\"\n",
        "\n",
        "# 검색 규칙 파라미터를 추가\n",
        "url_parameter_start = \"start=1\" \n",
        "url_parameter_display = \"&display=\" # start와 display는 검색결과를 얼만큼 보여줄지에 관련된 파라미터\n",
        "url_parameter_query = \"&query=홍대+고기집\" # 검색하고 싶은 장소나 음식점에 대한 검색어\n",
        "url_parameter_sorting = \"&sortingOrder=precision\" # 어떤 방식으로 검색 결과를 정렬할지에 대한 파라미터\n",
        "url_concat = source_url + url_parameter_start +             url_parameter_display + str(200) + url_parameter_query + url_parameter_sorting\n",
        "\n",
        "# 반환받은 API 데이터에 json.loads 함수를 사용\n",
        "json_data = requests.get(url_concat).text # requests.get 함수는 json 형태의 검색 결과 데이터를 얻을 수 있음\n",
        "restaurant_list_data = json.loads(json_data) #json.loads 함수는 파이썬의 딕셔너리 형태로 사용가능\n",
        "\n",
        "# 관련도순 상위 100개의 고기집 리스트 정보를 출력.\n",
        "print(str(restaurant_list_data)[:200]+\" 정보출력 끝.\")\n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "restaurant_id_list = []\n",
        "\n",
        "# 100개씩의 API 호출 결과를 3번, 마지막에 50번을 한번 더, 총 4번 가져옴. 크롤링 260회가 넘어가면 오류가 발생(이유모름)\n",
        "for start_idx in [1, 101, 201, 251]:\n",
        "    \n",
        "    # 네이버 플레이스 API를 호출하기 위한 기본 주소.\n",
        "    source_url = \"https://store.naver.com/sogum/api/businesses?\"\n",
        "    \n",
        "    # 검색 규칙 파라미터를 추가.\n",
        "    url_parameter_start = \"start=\" + str(start_idx)\n",
        "    url_parameter_display = \"&display=\"\n",
        "    url_parameter_query = \"&query=홍대+고기집\"\n",
        "    url_parameter_sorting = \"&sortingOrder=precision\"\n",
        "    url_concat = source_url + url_parameter_start +                 url_parameter_display + str(start_idx+199) + url_parameter_query + url_parameter_sorting\n",
        "    print(\"request_url:\", url_concat)\n",
        "    json_data = requests.get(url_concat).text\n",
        "    restaurant_list_data = json.loads(json_data)\n",
        "\n",
        "    # 크롤링에 필요한 각 리뷰 상세 페이지의 id를 추출.\n",
        "    for restaurant in restaurant_list_data['items']:\n",
        "        if 'moreBookingReviewsPath' in restaurant:\n",
        "            restaurant_id_list.append(restaurant['id'])\n",
        "\n",
        "restaurant_id_list = list(set(restaurant_id_list))\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "columns = ['score', 'review']\n",
        "df = pd.DataFrame(columns=columns)\n",
        "\n",
        "# 네이버 리뷰 상세 페이지의 기본 주소.\n",
        "source_url_head = \"https://store.naver.com/restaurants/detail?id=\"\n",
        "source_url_tail = \"&tab=bookingReview#_tab\"\n",
        "\n",
        "for idx in range(0, len(restaurant_id_list)):\n",
        "    print(\"Crawl\", str(int(idx/len(restaurant_id_list)*100)), \"% complete..\")\n",
        "    \n",
        "    # 앞서 추출한 리뷰 상세 페이지의 id를 기본 주소의 파라미터로 추가\n",
        "    req = requests.get(source_url_head + str(restaurant_id_list[idx]) + source_url_tail)\n",
        "    html = req.content\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    review_area = soup.find(name=\"div\", attrs={\"class\":\"review_area\"})\n",
        "\n",
        "    # 리뷰가 없는 페이지는 아무 작업도 수행하지 않음\n",
        "    if review_area is None:\n",
        "        continue\n",
        "\n",
        "    # 개발자 도구로 살펴본 html 구조에서 리뷰의 점수, 텍스트 부분을 추출\n",
        "    review_list = review_area.find_all(name=\"div\", attrs={\"class\":\"info_area\"})\n",
        "    for review in review_list:\n",
        "        score = review.find(name=\"span\", attrs={\"class\":\"score\"}).text\n",
        "        review_txt = review.find(name=\"div\", attrs={\"class\":\"review_txt\"}).text\n",
        "\n",
        "        # 추출한 리뷰의 점수, 리뷰 텍스트를 데이터프레임으로 병합\n",
        "        row = [score, review_txt]\n",
        "        series = pd.Series(row, index=df.columns)\n",
        "        df = df.append(series, ignore_index=True)\n",
        "print(\"Crawl 100 %\", \"complete\")\n",
        "\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "# 4점 이상의 리뷰는 긍정 리뷰, 3점 이하의 리뷰는 부정 리뷰로 평가\n",
        "df['y'] = df['score'].apply(lambda x: 1 if float(x) > 4 else 0)\n",
        "print(df.shape)\n",
        "df.head()\n",
        "\n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "df.to_csv(\"review_data.csv\", index=False)\n",
        "df = pd.read_csv(\"review_data.csv\")\n",
        "\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "#한글로 전처리하기\n",
        "\n",
        "import re\n",
        "\n",
        "# 텍스트 정제 함수 : 한글 이외의 문자는 전부 제거\n",
        "def text_cleaning(text):\n",
        "    # 한글의 정규표현식으로 한글만 추출.\n",
        "    hangul = re.compile('[^ ㄱ-ㅣ가-힣]+')\n",
        "    result = hangul.sub('', text)\n",
        "    return result\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "# 함수를 적용하여 리뷰에서 한글만 추출.\n",
        "df['ko_text'] = df['review'].apply(lambda x: text_cleaning(x))\n",
        "del df['review']\n",
        "df.head()\n",
        "\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "#형태소 단위로 추출 // KoNLPy 패키지 설치해야함 / https://ellun.tistory.com/46 에서참고했음\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# konlpy라이브러리로 텍스트 데이터에서 형태소를 추출.\n",
        "def get_pos(x):\n",
        "    tagger = Okt()\n",
        "    pos = tagger.pos(x)\n",
        "    pos = ['{}/{}'.format(word,tag) for word, tag in pos]\n",
        "    return pos\n",
        "\n",
        "# 형태소 추출 동작을 테스트.\n",
        "result = get_pos(df['ko_text'][0])\n",
        "print(result)\n",
        "\n",
        "\n",
        "# In[10]:\n",
        "\n",
        "\n",
        "#분류모델의 학습 데이터로 변환\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 형태소를 벡터 형태의 학습 데이터셋(X 데이터)으로 변환.\n",
        "index_vectorizer = CountVectorizer(tokenizer = lambda x: get_pos(x))\n",
        "X = index_vectorizer.fit_transform(df['ko_text'].tolist())\n",
        "\n",
        "\n",
        "# In[11]:\n",
        "\n",
        "\n",
        "X.shape #0000개의 피처를 가진 000개의 학습 데이터셋이 생성되었음\n",
        "\n",
        "\n",
        "# In[12]:\n",
        "\n",
        "\n",
        "print(str(index_vectorizer.vocabulary_)[:100]+\"..\")\n",
        "\n",
        "\n",
        "# In[13]:\n",
        "\n",
        "\n",
        "print(df['ko_text'][0])\n",
        "print(X[0])\n",
        "\n",
        "\n",
        "# In[15]:\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# TF-IDF 방법으로, 형태소를 벡터 형태의 학습 데이터셋(X 데이터)으로 변환\n",
        "tfidf_vectorizer = TfidfTransformer()\n",
        "X = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "\n",
        "# In[16]:\n",
        "\n",
        "\n",
        "print(X.shape)\n",
        "print(X[0])\n",
        "\n",
        "\n",
        "# In[17]:\n",
        "\n",
        "\n",
        "#긍정/부정 리뷰 분류 모델링 데이터셋 분리\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = df['y']\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "\n",
        "# In[18]:\n",
        "\n",
        "\n",
        "#분류 모델링 : 로지스틱 회귀 모델\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 로지스틱 회귀모델을 학습.\n",
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(x_train, y_train)\n",
        "y_pred = lr.predict(x_test)\n",
        "y_pred_probability = lr.predict_proba(x_test)[:,1]\n",
        "\n",
        "# 로지스틱 회귀모델의 성능을 평가.\n",
        "print(\"accuracy: %.2f\" % accuracy_score(y_test, y_pred))\n",
        "print(\"Precision : %.3f\" % precision_score(y_test, y_pred))\n",
        "print(\"Recall : %.3f\" % recall_score(y_test, y_pred))\n",
        "print(\"F1 : %.3f\" % f1_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "# In[19]:\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Confusion Matrix를 출력.\n",
        "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "print(confmat)\n",
        "\n",
        "\n",
        "# In[20]:\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# AUC를 계산.\n",
        "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_probability)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_probability)\n",
        "print(\"AUC : %.3f\" % roc_auc)\n",
        "\n",
        "# ROC curve 그래프를 출력\n",
        "plt.rcParams['figure.figsize'] = [5, 4]\n",
        "plt.plot(false_positive_rate, true_positive_rate, label='ROC curve (area = %0.3f)' % roc_auc, \n",
        "         color='red', linewidth=4.0)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC curve of Logistic regression')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "\n",
        "# In[21]:\n",
        "\n",
        "\n",
        "#위의 분류모델은 모든 데이터를 1로 예측, 모델이 하나의 결과만을 예측하도록 \n",
        "#잘못학습하여 클래스의 불균형 문제가 발생. \n",
        "#적절한 샘플링 방법을 통해 클래스의 불균형 문제를 해결\n",
        "\n",
        "\n",
        "# In[22]:\n",
        "\n",
        "\n",
        "#분류 모델 개선\n",
        "# y가 0과 1을 각각 얼마나 가지고 있는지를 출력\n",
        "df['y'].value_counts()\n",
        "\n",
        "\n",
        "# In[23]:\n",
        "\n",
        "\n",
        "# 1:1 비율로 랜덤 샘플링을 수행\n",
        "positive_random_idx = df[df['y']==1].sample(50, random_state=50).index.tolist()\n",
        "negative_random_idx = df[df['y']==0].sample(50, random_state=50).index.tolist()\n",
        "\n",
        "\n",
        "# In[24]:\n",
        "\n",
        "\n",
        "# 랜덤 데이터로 데이터셋을 나눕니다.\n",
        "random_idx = positive_random_idx + negative_random_idx\n",
        "sample_X = X[random_idx, :]\n",
        "y = df['y'][random_idx]\n",
        "x_train, x_test, y_train, y_test = train_test_split(sample_X, y, test_size=0.50)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "\n",
        "# In[25]:\n",
        "\n",
        "\n",
        "#Logistic Regression 다시 학습\n",
        "\n",
        "# 로지스틱 회귀모델을 다시 학습\n",
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(x_train, y_train)\n",
        "y_pred = lr.predict(x_test)\n",
        "y_pred_probability = lr.predict_proba(x_test)[:,1]\n",
        "\n",
        "# 학습한 모델을 테스트 데이터로 평가.\n",
        "print(\"accuracy: %.2f\" % accuracy_score(y_test, y_pred))\n",
        "print(\"Precision : %.3f\" % precision_score(y_test, y_pred))\n",
        "print(\"Recall : %.3f\" % recall_score(y_test, y_pred))\n",
        "print(\"F1 : %.3f\" % f1_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "# In[26]:\n",
        "\n",
        "\n",
        "# Confusion matrix를 출력.\n",
        "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "print(confmat)\n",
        "\n",
        "\n",
        "# In[27]:\n",
        "\n",
        "\n",
        "# AUC를 계산.\n",
        "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_probability)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_probability)\n",
        "print(\"AUC : %.3f\" % roc_auc)\n",
        "\n",
        "# ROC curve 그래프를 출력.\n",
        "plt.rcParams['figure.figsize'] = [5, 4]\n",
        "plt.plot(false_positive_rate, true_positive_rate, label='ROC curve (area = %0.3f)' % roc_auc, \n",
        "         color='red', linewidth=4.0)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC curve of Logistic regression')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "\n",
        "# In[28]:\n",
        "\n",
        "\n",
        "#중요 키워드 분석\n",
        "#회귀 모델의 피처 영향력 추출\n",
        "\n",
        "# 학습한 회귀 모델의 계수를 출력.\n",
        "plt.rcParams['figure.figsize'] = [10, 8]\n",
        "plt.bar(range(len(lr.coef_[0])), lr.coef_[0])\n",
        "\n",
        "\n",
        "# In[29]:\n",
        "\n",
        "\n",
        "print(sorted(((value, index) for index, value in enumerate(lr.coef_[0])), reverse=True)[:5])\n",
        "print(sorted(((value, index) for index, value in enumerate(lr.coef_[0])), reverse=True)[-5:])\n",
        "\n",
        "\n",
        "# In[30]:\n",
        "\n",
        "\n",
        "#중요 피처의 형태소\n",
        "\n",
        "\n",
        "# In[31]:\n",
        "\n",
        "\n",
        "# 회귀 모델의 계수를 높은 순으로 정렬. \n",
        "coef_pos_index = sorted(((value, index) for index, value in enumerate(lr.coef_[0])), reverse=True)\n",
        "\n",
        "\n",
        "# In[32]:\n",
        "\n",
        "\n",
        "# 회귀 모델의 계수를 index_vectorizer에 맵핑하여, 어떤 형태소인지 출력.\n",
        "invert_index_vectorizer = {v: k for k, v in index_vectorizer.vocabulary_.items()}\n",
        "\n",
        "# 계수가 높은 순으로, 피처에 형태소를 맵핑한 결과를 출력. 계수가 높은 피처는 리뷰에 긍정적인 영향을 주는 형태소라고 할 수 있습니다.\n",
        "print(str(invert_index_vectorizer)[:100]+'..')\n",
        "\n",
        "\n",
        "# In[33]:\n",
        "\n",
        "\n",
        "# 상위 20개 긍정 형태소를 출력\n",
        "for coef in coef_pos_index[:20]:\n",
        "    print(invert_index_vectorizer[coef[1]], coef[0])\n",
        "\n",
        "\n",
        "# In[34]:\n",
        "\n",
        "\n",
        "# 상위 20개 부정 형태소를 출력\n",
        "for coef in coef_pos_index[-20:]:\n",
        "    print(invert_index_vectorizer[coef[1]], coef[0])\n",
        "\n",
        "\n",
        "# In[35]:\n",
        "\n",
        "\n",
        "noun_list = []\n",
        "adjective_list = []\n",
        "\n",
        "# 명사, 형용사별로 계수가 높은 상위 10개의 형태소를 추출. 리뷰에 긍정적인 영향을 주는 명사와 형용사 순위별\n",
        "for coef in coef_pos_index[:100]:\n",
        "    pos_category = invert_index_vectorizer[coef[1]].split(\"/\")[1]\n",
        "    if pos_category == \"Noun\":\n",
        "        noun_list.append((invert_index_vectorizer[coef[1]], coef[0]))\n",
        "    elif pos_category == \"Adjective\":\n",
        "        adjective_list.append((invert_index_vectorizer[coef[1]], coef[0]))\n",
        "\n",
        "\n",
        "# In[36]:\n",
        "\n",
        "\n",
        "# 상위 10개의 명사를 출력\n",
        "noun_list[:10]\n",
        "\n",
        "\n",
        "# In[37]:\n",
        "\n",
        "\n",
        "# 상위 10개의 형용사를 출력\n",
        "adjective_list[:10]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}